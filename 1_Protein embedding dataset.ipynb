{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ea0b16-a5ec-467a-86c7-2309ae65e1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from biotite.database import rcsb\n",
    "from rcsbapi.search import search_attributes as attrs\n",
    "from Bio.PDB import PDBList\n",
    "\n",
    "import torch \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70150124-8d8f-4d9e-a654-352239d22fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "login(HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b8d9bd-7f5a-4a4f-9f21-f48055761ab6",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3bb886-ec6e-45e2-924e-521ccc2de7e3",
   "metadata": {},
   "source": [
    "### 1. Get all PDBs from Huggingface dataset `pdb_protein_ligand_complexes` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af029e7a-933a-4b01-b3c1-6355c4f44ed2",
   "metadata": {},
   "source": [
    "Download from here https://huggingface.co/datasets/jglaser/pdb_protein_ligand_complexes  \n",
    "and put in `data/` directory.  \n",
    "I removed unnecessary columns and saved the new datasets to: `pdb_protein_ligand_train.p` and `pdb_protein_ligand_test.p` which are much smaller. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8669783b-9e08-42ca-b220-4ad205dd78ea",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9daf770-044f-48fb-8486-d4f9ff39e90a",
   "metadata": {},
   "source": [
    "### 2. Apply **ESM** (via **Synthyra**) to PDB IDs to get protein embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c3d1e1-1098-472d-b5ed-f3d63cf94116",
   "metadata": {},
   "source": [
    "Source: https://huggingface.co/Synthyra/ESMplusplus_large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cb1ca3-81c8-44df-8427-5ed1f6310fc2",
   "metadata": {},
   "source": [
    "#### 2.1. Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63f4716-d307-48db-86a7-5b199248a23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDB: drugbank_target, drugbank_info, drugbank_container_identifiers ???\n",
    "# PDB: ligands ????\n",
    "# attrs.rcsb_binding_affinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8178a782-c4ea-4d06-be20-90da10fff908",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"data/pdb_sequences.csv\"):\n",
    "    df = pd.read_csv(\"data/pdb_sequences.csv\", index_col=False)\n",
    "    print(len(df), \"PDB sequences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfe0618-211a-4bb4-9661-699b6e16100c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_pickle(\"data/pdb_protein_ligand_train.p\")[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d7d62a-e5ac-473a-a52a-bd2fd8a0b698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: filter df to only include proteins with ligand \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b713286-e6e9-41f6-b37f-53de46927191",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e69dd6-bee5-4df4-800a-626a88c7b67a",
   "metadata": {},
   "source": [
    "#### 2.2. Embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8f4738-e68c-407a-87fb-616e176e0a5a",
   "metadata": {},
   "source": [
    "Using `Synthyra/ESMplusplus` via `transformers` library, as it is easier to run batched inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd491406-b32b-4c23-9559-554685939525",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cf6e59-b78b-4cba-824d-036c7dd78f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect device: CUDA or CPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6b440a-ec25-4aea-a9fd-4a9f694b7aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthyra ESM models:\n",
    "#   ESMplusplus_large: corresponds to ESM-C 600m \n",
    "#   ESMplusplus_small: corresponds to ESM-C 300m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7573bb-f895-4aed-89a3-744969440b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(\"Synthyra/ESMplusplus_small\", trust_remote_code=True)\n",
    "model = model.to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1fa908-2f9a-4c4b-a11c-4293407a9794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for a small batch (10 proteins): \n",
    "\n",
    "x_sequences = df[\"sequence\"][:10].values\n",
    "tokenized_sequences = model.tokenizer(x_sequences.tolist(), padding=True, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(**tokenized_sequences)  # get ALL hidden states by setting output_hidden_states=True\n",
    "\n",
    "y_embeddings = output.last_hidden_state\n",
    "y_embeddings.shape \n",
    "# (batch_size, seq_len, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc15d0a8-193c-4bbc-a8d1-7736b58f7bc6",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89f5d42-06d5-47fa-8c09-d4ff08b7cf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the dataframe into batches of size batch_size\n",
    "batch_size = 100\n",
    "num_batches = int(len(df)/batch_size)\n",
    "print(num_batches, \"batches\")\n",
    "\n",
    "# TODO: do proper torch dataset object\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f8920f-125f-4ccc-bbeb-cd3e0694f780",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"data/_esm\"):\n",
    "    os.mkdir(\"data/_esm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385377df-5c6f-44b2-a8b4-ad3eae68b7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "x_ids = []\n",
    "y_embeddings = []\n",
    "\n",
    "for i_batch in range(num_batches):\n",
    "    print(\"Batch\", i_batch)\n",
    "    \n",
    "    batch_df = df.iloc[i_batch*batch_size:i_batch*batch_size+batch_size]\n",
    "    \n",
    "    # Combined (pdb_id + chain_id) as identifier: \n",
    "    x_ids = (batch_df[\"pdb_id\"] + \"_\" + batch_df[\"chain_id\"]).values  \n",
    "    x_sequences = batch_df[\"sequence\"].values\n",
    "    \n",
    "    tokenized_sequences = model.tokenizer(\n",
    "        x_sequences.tolist(), \n",
    "        padding=True, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(**tokenized_sequences)  \n",
    "    \n",
    "    y_embeddings = output.last_hidden_state \n",
    "    \n",
    "    # Save to file:\n",
    "    # I will save each batch separately for now, and deal with concatenation in a later step \n",
    "    with open(f\"data/_esm/esm_embeddings_batch_{i_batch}.pkl\", \"wb\") as f:\n",
    "        pickle.dump((x_ids, y_embeddings), f) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb5ba45-6f42-4bf4-9d3d-af8495277786",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7d9028-17d1-4cbc-a513-7ad5e64ebb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to check, load data for batch 0: \n",
    "\n",
    "with open(\"data/_esm/esm_embeddings_batch_0.pkl\", \"rb\") as f:\n",
    "    x_ids, y_embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad231e19-0f01-493b-b7b0-b63e8bf1f3b0",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868433b2-d69e-4c74-980b-ed0a7758d92e",
   "metadata": {},
   "source": [
    "Optionally: combine all batch files into a single dataset file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746ba520-6911-4335-9c16-aae618347f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Would be preferable for the next steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca26e018-08ec-406a-bd70-4e32402a4d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
