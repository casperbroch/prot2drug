{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5a7843d-26a5-4932-a2e3-5bda860cc246",
   "metadata": {},
   "source": [
    "### Apply SynFormer to the SMILES strings to generate the dataset of synthesis tokens "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88859b51-cc2f-4230-adc3-40650f1f88fc",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">  \n",
    "By this point, you already need the Huggingface dataset `pdb_protein_ligand_complexes` in the form of `pdb_protein_ligand_train.p` and `pdb_protein_ligand_test.p`. \n",
    "</pre>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3225849-e655-4b70-86af-69e8f50c5822",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">  \n",
    "The following code is based on the sample_naive.py script, i.e. no parallelization whatsoever. To make more efficient, rewrite based on scripts/sample.py instead of scripts/sample_naive.py \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24eff562-624f-482c-bf9c-2d0a797ff1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want a dataset (lig_id, original_smiles, projected_smiles), and then possibly multiple projections \n",
    "# per lig_id entry, so (lig_id, original_smiles, projected_smiles, projection_id) \n",
    "# But besides the SMILES, we need all 3 outputs from SynFormer: token type, reaction token, reactant token \n",
    "# If I remember correctly, there is a vocabulary of reaction tokens, so it would be enough to store the reaction token IDs,\n",
    "# which we can look up in the \n",
    "# I'm not sure if we can do the same with the reactants, as there is no vocabulary. But from the ID, perhaps we can\n",
    "# get the fingerprint and then at will we can pass it through the embedding layer to get the embedding vector \n",
    "# Because if we store the entire embedding vector for each reactant in all examples, that dataset would be unnecessarily big\n",
    "# Ideally, we just keep the IDs \n",
    "# We want the ground-truth embeddings anyway, not the predicted embeddings which would then be Nearest-Neighbor'ed to the ground truth embedding \n",
    "# So now the dataset would look like this: \n",
    "\n",
    "# (lig_id, original_smiles, projected_smiles, projection_id, token_types_tensor, reactions_tensor, reactants_tensor) \n",
    "\n",
    "# the tensors all seem to have a fixed sequence length of 24 \n",
    "# I could also simply store all non-END and non-START tokens, and then during the prediction I prepend/append them, as they're always the same \n",
    "\n",
    "# optionally, I could also store the similarity score between the original and projected molecules "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc61ab7-9bed-4270-b612-cf2dc55d72d9",
   "metadata": {},
   "source": [
    "The following code is partly based on `scripts/sample_naive.py` form the Synformer repo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7e92a7-68a9-4949-8dea-16cc78563e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports from scripts/sample_naive.py: \n",
    "import pathlib\n",
    "import pickle\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "from synformer.chem.fpindex import FingerprintIndex\n",
    "from synformer.chem.matrix import ReactantReactionMatrix\n",
    "from synformer.chem.mol import Molecule\n",
    "from synformer.models.synformer import Synformer\n",
    "\n",
    "# My own imports:\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from synformer.scripts.sample_naive import load_model, featurize_smiles \n",
    "# from synformer.models.synformer import draw_generation_results\n",
    "from synformer.data.common import TokenType\n",
    "from datetime import datetime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183bb887-2fe3-47b4-82cf-aa96d3b42a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"data/trained_weights/sf_ed_default.ckpt\"\n",
    "CONFIG_PATH = None \n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca3fa08-2c5c-4929-913e-e21d61a7a1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, fpindex, rxn_matrix = load_model(MODEL_PATH, CONFIG_PATH, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca1e5a4-779c-49ff-83bc-0bcfad94163f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synthetic_pathway(smiles, lig_id=None, repeat=1):\n",
    "    \"\"\"For a list of smiles\"\"\"\n",
    "    data = []\n",
    "    mol, feat = featurize_smiles(smiles, DEVICE, repeat=repeat)\n",
    "    with torch.inference_mode():\n",
    "        result = model.generate_without_stack(\n",
    "            feat,\n",
    "            rxn_matrix=rxn_matrix,\n",
    "            fpindex=fpindex,\n",
    "            temperature_token=1.0,\n",
    "            temperature_reactant=0.1,\n",
    "            temperature_reaction=1.0,\n",
    "        )\n",
    "        ll = model.get_log_likelihood(\n",
    "            code=result.code,\n",
    "            code_padding_mask=result.code_padding_mask,\n",
    "            token_types=result.token_types,\n",
    "            rxn_indices=result.rxn_indices,\n",
    "            reactant_fps=result.reactant_fps,\n",
    "            token_padding_mask=result.token_padding_mask,\n",
    "        )\n",
    "    stacks = result.build() \n",
    "    for i, stack in enumerate(stacks):\n",
    "        # Only those sequences with stack depth of 1 (i.e. applying the building blocks and reactions leads to 1 final molecule) \n",
    "        # are considered valid results!! \n",
    "        if stack.get_stack_depth() == 1:\n",
    "            analog_mol = stack.get_one_top()\n",
    "            sim = analog_mol.sim(mol)\n",
    "            # TODO: perhaps only continue if similarity score sufficiently high? \n",
    "            token_types = result.token_types[i]\n",
    "            # Location of first END token (we only need to store tokens up until this point): \n",
    "            end_id = token_types.tolist().index(0)\n",
    "            token_types = token_types[:end_id].tolist()\n",
    "            rxn_indices = result.rxn_indices[i,:end_id].tolist()\n",
    "            reactant_indices = result.reactant_indices[i,:end_id].tolist()\n",
    "            data.append([\n",
    "                lig_id,\n",
    "                smiles, \n",
    "                analog_mol.smiles, \n",
    "                round(sim, 4), \n",
    "                token_types,\n",
    "                rxn_indices,\n",
    "                reactant_indices\n",
    "            ])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae64e5f-7040-4c58-bd43-692cdd19cd97",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0487a0d7-8822-4eb8-bb9f-ccad68710d4f",
   "metadata": {},
   "source": [
    "Apply to dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4045852-87a2-412c-8449-729bd97569e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load lists of unique proteins and ligands: \n",
    "\n",
    "protein_ligand_train = pd.read_pickle(\"data/pdb_protein_ligand_train.p\")\n",
    "protein_ligand_test = pd.read_pickle(\"data/pdb_protein_ligand_test.p\")\n",
    "\n",
    "if os.path.exists(\"data/pdb_protein_ligand__unique_ligands.csv\"):\n",
    "    unique_proteins = pd.read_csv(\"data/pdb_protein_ligand__unique_ligands.csv\")\n",
    "else:\n",
    "    ligands_train = protein_ligand_train[[\"lig_id\", \"smiles\"]].drop_duplicates()\n",
    "    ligands_test = protein_ligand_test[[\"lig_id\", \"smiles\"]].drop_duplicates()\n",
    "    unique_ligands = pd.concat([ligands_train, ligands_test]).drop_duplicates()\n",
    "    unique_ligands.to_csv(\"data/pdb_protein_ligand__unique_ligands.csv\")\n",
    "\n",
    "if os.path.exists(\"data/pdb_protein_ligand__unique_proteins.csv\"):\n",
    "    unique_proteins = pd.read_csv(\"data/pdb_protein_ligand__unique_proteins.csv\")\n",
    "else:\n",
    "    proteins_train = protein_ligand_train[[\"pdb_id\", \"seq\"]]\n",
    "    proteins_test = protein_ligand_test[[\"pdb_id\", \"seq\"]]\n",
    "    unique_proteins = pd.concat([proteins_train, proteins_test]).drop_duplicates()\n",
    "    unique_proteins.to_csv(\"data/pdb_protein_ligand__unique_proteins.csv\")\n",
    "    \n",
    "del protein_ligand_train\n",
    "del protein_ligand_test\n",
    "\n",
    "print(ligands_train.shape[0], \"unique ligands\")\n",
    "print(ligands_test.shape[0], \"unique ligands\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c6058f-274a-4f37-b1ab-186c76b4cd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "data = {}\n",
    "repeat = 2\n",
    "\n",
    "# for ligands, dataset_name in [(ligands_test, \"test\"), (ligands_train, \"train\")]:\n",
    "ligands = ligands_test\n",
    "dataset_name = \"test\" \n",
    "data[dataset_name] = []\n",
    "\n",
    "for i, row in ligands[:10].iterrows():\n",
    "    try:\n",
    "        lig_id = row[\"lig_id\"].item() \n",
    "        smiles = row[\"smiles\"]\n",
    "        print(i, lig_id, smiles)\n",
    "        records = get_synthetic_pathway(smiles, lig_id=lig_id, repeat=repeat)\n",
    "        for record in records:\n",
    "            if record not in data[dataset_name]:\n",
    "                data[dataset_name].append(record)\n",
    "    except Exception as e:\n",
    "        # raise e \n",
    "        print(f\"Error processing SMILES {i} ({e})\")\n",
    "pickle.dump(\n",
    "    data[dataset_name], \n",
    "    open(f\"data/synformer_ligands_{dataset_name}_{datetime.now().strftime('%Y-%m-%d %H-%M-%S')}.pkl\", \"wb\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19300a20-7663-4744-8100-2b6b8bf6676d",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"lig_id\", \"smiles_original\", \"smiles_proj\", \"similarity\", \"token_types\", \"rxn_indices\", \"reactant_indices\"]\n",
    "\n",
    "df = pd.DataFrame(data[\"test\"], columns=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b526d695-2b0f-484e-8fa0-ce8a27d0f71e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91e8fa6-889f-4dfd-b23e-47845701e290",
   "metadata": {},
   "source": [
    "Just for convenience: reconstructing the sequence of building blocks and reactions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64847e7c-1cbb-4d42-b9df-ef47701dfe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_pathway(token_types, rxn_indices, reactant_indices):\n",
    "    \"\"\"\n",
    "    From a list of token types, reaction indices and reactant indices, construct the synthetic pathway\n",
    "    E.g. [1, 3], [0, 99], [0, 10]\n",
    "         This would be mapped to [START, B10, END]\n",
    "         At seq index 0, we have token type 1, which corresponds to the START token, so that's the first token \n",
    "         At seq index 1, we have token type 3, which corresponds to a REACTANT token, which we fetch from reactant_indices\n",
    "                         and in this case is token ID 10, so the token is \"B10\"\n",
    "         That's all of them. \n",
    "         At the end, I append an END token.\n",
    "    Token types:\n",
    "      0: END token (also used for padding, following the actual END token) \n",
    "      1: START token \n",
    "      2: REACTION\n",
    "      3: REACTANT\n",
    "    \"\"\"\n",
    "    pathway = []\n",
    "    for i, token_type in enumerate(token_types):\n",
    "        match token_type:\n",
    "            case 1:\n",
    "                token = \"START\"\n",
    "            case 2:\n",
    "                token = f\"R{rxn_indices[i]}\"\n",
    "            case 3:\n",
    "                token = f\"B{reactant_indices[i]}\"\n",
    "            case _:\n",
    "                token = None \n",
    "        pathway.append(token)\n",
    "    pathway.append(\"END\")\n",
    "    return pathway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e450a79e-1af8-40fd-9347-2cc7f2daebe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in df.iloc[:10].iterrows():\n",
    "    print(reconstruct_pathway(row[\"token_types\"], row[\"rxn_indices\"], row[\"reactant_indices\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cab138d-1b40-4db2-9f1e-4db54df5bb8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
