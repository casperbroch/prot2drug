{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77ea0b16-a5ec-467a-86c7-2309ae65e1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/janikeuskirchen/miniforge3/envs/prot2drug/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64abf425-496b-449b-bfb4-53388498a609",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "login(HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3561173-4602-45ab-8b91-0b2c90d29333",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54f35f09-3abc-454a-ac0e-bb830e99a9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from esm.models.esmc import ESMC\n",
    "# from esm.models.esm3 import ESM3\n",
    "from esm.sdk.api import ESMProtein, LogitsConfig  # , GenerationConfig\n",
    "\n",
    "from esm.utils.structure.protein_chain import ProteinChain\n",
    "from biotite.database import rcsb\n",
    "from rcsbapi.search import search_attributes as attrs\n",
    "from Bio.PDB import PDBList\n",
    "\n",
    "import torch \n",
    "import numpy as np\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b8d9bd-7f5a-4a4f-9f21-f48055761ab6",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7a404e-786c-4433-8a3a-aaa5592b9b01",
   "metadata": {},
   "source": [
    "### 1. Getting all PDBs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d1893ec-042d-47ef-8ab9-b049c2bd53be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228524 PDB IDs\n"
     ]
    }
   ],
   "source": [
    "# The following gets all PDB entries, but this contains non-proteins which ESM can't handle\n",
    "\"\"\"\n",
    "if os.path.exists(\"data/pdb_ids.csv\"):\n",
    "    all_protein_ids = pd.read_csv(\"data/pdb_ids.csv\").values.squeeze().tolist()\n",
    "    print(len(all_protein_ids), \"PDB IDs\")\n",
    "else:\n",
    "    pdbl = PDBList()\n",
    "    all_protein_ids = pdbl.get_all_entries()\n",
    "    # all_protein_ids = [\"1CM4\"]\n",
    "    print(len(all_protein_ids), \"PDB IDs\")\n",
    "    pd.Series(all_protein_ids).to_csv(\"data/pdb_ids.csv\", index=False)\n",
    "\"\"\"\n",
    "\n",
    "# Get only proteins from PDB\n",
    "if os.path.exists(\"data/pdb_ids.csv\"):\n",
    "    all_protein_ids = pd.read_csv(\"data/pdb_ids.csv\").values.squeeze().tolist()\n",
    "    print(len(all_protein_ids), \"PDB IDs\")\n",
    "else:\n",
    "    # pdbl = PDBList()\n",
    "    # all_protein_ids = pdbl.get_all_entries()\n",
    "    q = (attrs.rcsb_entry_info.polymer_entity_count_protein > 0)\n",
    "    all_protein_ids = list(q())\n",
    "    # all_protein_ids = [\"1CM4\"]\n",
    "    print(len(all_protein_ids), \"PDB IDs\")\n",
    "    pd.Series(all_protein_ids).to_csv(\"data/pdb_ids.csv\", index=False)\n",
    "\n",
    "# Alternatively: \n",
    "# Download this file https://ftp.ebi.ac.uk/pub/databases/pdb/derived_data/pdb_entry_type.txt\n",
    "# and then filter by type \"prot\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5204f6fb-0db2-4d80-8f02-12253c82433a",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cded6b-e988-4d4d-aa85-34bc9059a18b",
   "metadata": {},
   "source": [
    "### 2A. Getting the protein embeddings for all PDB IDs using **`esm`** library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a851253e-3727-41a5-9981-df2fa7544f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem: there doesn't appear to be a good way to pass a whole batch of proteins!! \n",
    "# So we would have to run all proteins sequentially ... which is not desirable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747aaf5f-f52a-4c57-91cd-f0af07243625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# esmc = ESMC.from_pretrained(\"esmc_300m\")\n",
    "# esm3 = ESM3.from_pretrained(\"esm3-open\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a179821f-56e2-49ec-9e6a-bc7a55800102",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "%%time \n",
    "\n",
    "protein_ids = []\n",
    "protein_embeddings = [] \n",
    "\n",
    "for protein_id in all_protein_ids: \n",
    "    protein_chain = ProteinChain.from_pdb(rcsb.fetch(protein_id, \"pdb\")) # , chain_id=\"A\") \n",
    "    # Get protein object with all the ground-truth data (except function for some reason) \n",
    "    # In the code, they don't provide a way to automatically fetch function annotations, \n",
    "    # instead I have to fetch them myself and then set protein.function_annotations \n",
    "    # known_protein = ESMProtein.from_protein_chain(protein_chain) \n",
    "    # Get protein with just the sequence data \n",
    "    protein = ESMProtein(sequence=protein_chain.sequence) \n",
    "    # I don't think we can put all tokens into a batch to run through the model at once? \n",
    "    protein_tensor = esmc.encode(protein)\n",
    "    output = esmc.logits(\n",
    "        protein_tensor, \n",
    "        LogitsConfig(\n",
    "            return_hidden_states=True,  # !!\n",
    "            # ESMC-300m has 30 layers, so final layer is at index 29:\n",
    "            ith_hidden_layer=29\n",
    "        )\n",
    "    )\n",
    "    protein_ids.append(protein_id)\n",
    "    protein_embeddings.append(output.hidden_states.squeeze())\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8669783b-9e08-42ca-b220-4ad205dd78ea",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9daf770-044f-48fb-8486-d4f9ff39e90a",
   "metadata": {},
   "source": [
    "### 2B. Getting the protein embeddings for all PDB IDs using **`huggingface`** and **Synthyra** implementations of ESM "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c3d1e1-1098-472d-b5ed-f3d63cf94116",
   "metadata": {},
   "source": [
    "Source: https://huggingface.co/Synthyra/ESMplusplus_large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cb1ca3-81c8-44df-8427-5ed1f6310fc2",
   "metadata": {},
   "source": [
    "#### 2B.1. Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "767109f1-17c5-4c3f-8229-bd779d1d8fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442cca36-903a-4941-86a0-6fb7634f9e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this approach, we need all the proteins' sequences\n",
    "# We could just do this:\n",
    "# ProteinChain.from_pdb(rcsb.fetch(protein_id, \"pdb\")).sequence \n",
    "# but we would have to do it for each protein sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aafd11d-cf57-4f4d-903a-e8e0a69a146e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Let's try it by running the API queries to PDB (RCSB) directly \n",
    "\n",
    "batch_size = 5000\n",
    "batches = [all_protein_ids[i:i+batch_size] for i in range(0, len(all_protein_ids), batch_size)]\n",
    "print(len(batches), \"batches\")\n",
    "\n",
    "batches = [batches[0]]\n",
    "\n",
    "results = []\n",
    "\n",
    "# Loop through batches\n",
    "for i, batch in enumerate(batches, start=1):\n",
    "    print(f\"Processing batch {i}/{len(batches)}\")\n",
    "    query = DataQuery(\n",
    "        input_type=\"entries\",\n",
    "        input_ids=all_protein_ids,\n",
    "        return_data_list=[\n",
    "            \"rcsb_id\",\n",
    "            \"polymer_entities.entity_poly.rcsb_entity_polymer_type\",\n",
    "            \"polymer_entities.entity_poly.pdbx_seq_one_letter_code_can\"\n",
    "        ]\n",
    "    )\n",
    "    try:\n",
    "        # Execute the query for this batch\n",
    "        batch_results = query.exec()\n",
    "        json.dump(batch_results, open(f\"data/pdb_sequences_batch_{i}.json\", \"w\"))\n",
    "        results.extend(batch_results)\n",
    "        # Small delay to avoid rate limits\n",
    "        time.sleep(1)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed batch {i}: {e}\")\n",
    "        continue \n",
    "\n",
    "json.dump(results, open(f\"data/pdb_sequences.json\", \"w\"))\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63f4716-d307-48db-86a7-5b199248a23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDB: drugbank_target, drugbank_info, drugbank_container_identifiers ???\n",
    "# PDB: ligands ????\n",
    "# attrs.rcsb_binding_affinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2fe6013-a734-468f-9110-cc9086e1fe8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download `pdb_seqres.txt` from here: https://ftp.ebi.ac.uk/pub/databases/pdb/derived_data/ \n",
    "\n",
    "if not os.path.exists(\"data/pdb_seqres.txt\"):\n",
    "    response = requests.get(\"https://ftp.ebi.ac.uk/pub/databases/pdb/derived_data/pdb_seqres.txt\")\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"There was a problem (status code {response.status_code}). Please try again.\")\n",
    "    else:\n",
    "        with open(\"data/pdb_seqres.txt\", \"w\") as f:\n",
    "            f.write(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8178a782-c4ea-4d06-be20-90da10fff908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166982 PDB sequences\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"data/pdb_sequences.csv\"):\n",
    "    df = pd.read_csv(\"data/pdb_sequences.csv\", index_col=False)\n",
    "    print(len(df), \"PDB sequences\")\n",
    "else:\n",
    "    if not os.path.exists(\"data/pdb_seqres.txt\"):\n",
    "        raise Exception(\"Please download pdb_seqres.txt from https://ftp.ebi.ac.uk/pub/databases/pdb/derived_data/ and put it in the data/ directory.\")\n",
    "    with open(\"data/pdb_seqres.txt\") as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    sequences = []\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        if \"mol:protein\" in line:\n",
    "            raw = line[1:].strip().split()\n",
    "            pdb_id, chain_id = raw[0].upper().split(\"_\")\n",
    "            # mol_type = raw[1].split(\":\")[1]\n",
    "            length = raw[2].split(\":\")[1]\n",
    "            name = raw[3]\n",
    "            record = {\n",
    "                \"pdb_id\": pdb_id,\n",
    "                \"chain_id\": chain_id,\n",
    "                # \"type\": mol_type,\n",
    "                \"length\": length,\n",
    "                \"name\": name,\n",
    "                \"sequence\": lines[i+1].strip()\n",
    "            }\n",
    "            sequences.append(record)\n",
    "    \n",
    "    df = pd.DataFrame(sequences)\n",
    "    df = df[df[\"pdb_id\"].isin(all_protein_ids)]\n",
    "    df = df.drop_duplicates(\"sequence\").reset_index(drop=True)\n",
    "    print(len(df), \"PDB sequences\")\n",
    "    df.to_csv(\"data/pdb_sequences.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b713286-e6e9-41f6-b37f-53de46927191",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e69dd6-bee5-4df4-800a-626a88c7b67a",
   "metadata": {},
   "source": [
    "#### 2B.2. Embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8f4738-e68c-407a-87fb-616e176e0a5a",
   "metadata": {},
   "source": [
    "Using `Synthyra/ESMplusplus` via `transformers` library, as it is easier to run batched inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd491406-b32b-4c23-9559-554685939525",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6b440a-ec25-4aea-a9fd-4a9f694b7aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthyra ESM models:\n",
    "#   ESMplusplus_large: corresponds to ESM-C 600m \n",
    "#   ESMplusplus_small: corresponds to ESM-C 300m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41cf6e59-b78b-4cba-824d-036c7dd78f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Detect device: CUDA or CPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e7573bb-f895-4aed-89a3-744969440b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(\"Synthyra/ESMplusplus_small\", trust_remote_code=True)\n",
    "model = model.to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c1fa908-2f9a-4c4b-a11c-4293407a9794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 169, 960])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test for a small batch (10 proteins): \n",
    "\n",
    "x_sequences = df[\"sequence\"][:10].values\n",
    "tokenized_sequences = model.tokenizer(x_sequences.tolist(), padding=True, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(**tokenized_sequences)  # get ALL hidden states by setting output_hidden_states=True\n",
    "\n",
    "y_embeddings = output.last_hidden_state\n",
    "y_embeddings.shape \n",
    "# (batch_size, seq_len, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d89f5d42-06d5-47fa-8c09-d4ff08b7cf64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166982 batches\n"
     ]
    }
   ],
   "source": [
    "# Divide the dataframe into batches of size batch_size\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "num_batches = int(len(df)/batch_size)\n",
    "print(num_batches, \"batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69f8920f-125f-4ccc-bbeb-cd3e0694f780",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"data/esm_embeddings\"):\n",
    "    os.mkdir(\"data/esm_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "385377df-5c6f-44b2-a8b4-ad3eae68b7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "CPU times: user 633 ms, sys: 110 ms, total: 743 ms\n",
      "Wall time: 196 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "x_ids = []\n",
    "y_embeddings = []\n",
    "\n",
    "for i_batch in range(num_batches):\n",
    "    print(\"Batch\", i_batch)\n",
    "    \n",
    "    batch_df = df.iloc[i_batch*batch_size:i_batch*batch_size+batch_size]\n",
    "    \n",
    "    # Combined (pdb_id + chain_id) as identifier: \n",
    "    x_ids = (batch_df[\"pdb_id\"] + \"_\" + batch_df[\"chain_id\"]).values  \n",
    "    x_sequences = batch_df[\"sequence\"].values\n",
    "    \n",
    "    tokenized_sequences = model.tokenizer(\n",
    "        x_sequences.tolist(), \n",
    "        padding=True, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(**tokenized_sequences)  # get ALL hidden states by setting output_hidden_states=True\n",
    "    \n",
    "    y_embeddings = output.last_hidden_state \n",
    "    \n",
    "    # Save to file:\n",
    "    # I will save each batch separately for now, and deal with concatenation in a later step \n",
    "    with open(f\"data/esm_embeddings/esm_embeddings_batch_{i_batch}.pkl\", \"wb\") as f:\n",
    "        pickle.dump((x_ids, y_embeddings), f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d7d9028-17d1-4cbc-a513-7ad5e64ebb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E.g. load data for batch 0 \n",
    "with open(\"data/esm_embeddings/esm_embeddings_batch_0.pkl\", \"rb\") as f:\n",
    "    x_ids, y_embeddings = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
